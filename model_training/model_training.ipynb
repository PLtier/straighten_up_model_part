{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upewnij się, że jesteś w odrębnym środowisku od środowiska używanego do konwersji modelu na model TFJS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ten plik pokazuje krok po kroku w jaki sposób była trenowana ta sieć. W kodzie wprowadziłem stosowne komentarze po angielsku."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpierw są pobierane odpowiednie biblioteki oraz ustawiony seed dla maksymalnej reprodukowalności badań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5lLZVqQnyH3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, matthews_corrcoef\n",
    "\n",
    "#In order to main reproducibility\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "# Check if you have properly installed GPU support\n",
    "# tf.config.list_physical_devices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz ustawiane są 3 zmienne:\n",
    "- jakie punkty chcemy by model brał pod uwagę.\n",
    "- z ilu powtórzeń trenowania modelu chcemy liczyć średnią metryk.\n",
    "- jaki jest próg klasyfikacji klas na pozytywną lub negatywną. Powyżej - pozytywna. Poniżej - negatywna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_configuration={\n",
    "    'all': 17,\n",
    "    'head_shoulders': 7,\n",
    "    'head_shoulders_hands': 11,\n",
    "\n",
    "}\n",
    "\n",
    "#How many keypoints we want to use\n",
    "choose_configuration='all'\n",
    "keypoints_amount=keypoints_configuration[choose_configuration]\n",
    "\n",
    "#How many times repeat fitting the model in order to get average of metrics\n",
    "#set 1 or 2 when want to quickly check if everything works\n",
    "repeat=10\n",
    "\n",
    "#Set threshold when evaluating the model. Above it - improper posture, positive class. \n",
    "#Below it - proper posture, negative class\n",
    "threshold=0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załadowanie zdjęć/Datasetu. Domyślnie ładuje się gotowy zbiór punktów.\n",
    "\n",
    "Uwaga! Jeśli chcesz korzystać z własnego datasetu zdjęć proszę upewnić się, że:\n",
    "- rozpakowano pliki odpowiednio (wykorzystaj ```extract_files.ipynb```).\n",
    "- odkomentowano poniższy kod, a zakomentowano 2 pierwsze linijki\n",
    "- ustawiano zmienną ```samples_path``` tak by wskazywała na folder z 2 folderami zawierającymi zdjęcia z klasami. Domyślnie ustawia \"classified_images\"\n",
    "- nazwa folderu ze zdjęciami z nieodpowiednią posturą (klasa pozytywna) alfabetycznie jest druga od nazwy folderu ze zdjęciami z odpowiednią posturą (klasa negatywna). Można umieścić zdjęcia bezpośrednio do odpowiednich folderów.\n",
    "\n",
    "Program trenujący model na zdjęciach w trakcie przygotowywania mojej pracy zabierał ok. 40 minut czasu, więc w przypadku umieszczenia własnego, o podobnej skali datasetu, należy się spodziewać dłuższego oczekiwania. przeciwnym razie skorzystaj z gotowego datasetu, który zawiera od razu zdjęcia przekonwertowane na punkty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path=\"../ready_components/keypoints_dataset/\"\n",
    "keypoints_dataset = tf.data.Dataset.load(dataset_path)\n",
    "\"\"\"\n",
    "samples_path = \"./classified_images/\"\n",
    "#Initialize needed variables\n",
    "\n",
    "model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "movenet = model.signatures['serving_default']\n",
    "#Set source of samples\n",
    "\n",
    "#Important note: your positive class should be in a folder which is alphabetically second, otherwise \n",
    "#it will be treated as a negative class, which is not what we want\n",
    "image_dataset = keras.utils.image_dataset_from_directory(\n",
    "    directory=samples_path,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=None,\n",
    "    shuffle=True,\n",
    "    image_size=(256, 256),\n",
    "    seed=42)\n",
    "\n",
    "def translate_into_points(image):\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    image = tf.cast(image, dtype=tf.int32)\n",
    "    output=tf.squeeze(movenet(image)['output_0'])\n",
    "    return output;\n",
    "\n",
    "\n",
    "def map_into(filepath, label):\n",
    "    return translate_into_points(filepath), label\n",
    "\n",
    "keypoints_dataset=image_dataset.map(map_into)\n",
    "print(keypoints_dataset)\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nieinteresujące punkty są eliminowane z datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if(choose_configuration != 'all'):\n",
    "    def extract_from_head_to_wrists(keypoints, label):\n",
    "        keypoints = keypoints[:keypoints_amount]\n",
    "        return keypoints, label\n",
    "\n",
    "    keypoints_dataset = keypoints_dataset.map(extract_from_head_to_wrists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(keypoints_dataset))\n",
    "keypoints_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset jest dzielony na część treningową, walidacyjną i testową w proporcjach 60/20/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning! This step below can get above 30min to complete if using custom dataset!\n",
    "\n",
    "#Making 60/20/20 split\n",
    "train_dataset, test_dataset = tf.keras.utils.split_dataset(keypoints_dataset, left_size=0.8, shuffle=True, seed=42)\n",
    "train_dataset, validation_dataset = keras.utils.split_dataset(train_dataset, right_size=0.25, shuffle=True, seed=42)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(validation_dataset))\n",
    "train_dataset = train_dataset.batch(100)\n",
    "test_dataset = test_dataset.batch(100)\n",
    "validation_dataset = validation_dataset.batch(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utworzony jest model klasyfikujący. Hiperparametr learning rate wynosi 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hECXa1WOn994"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_and_compile_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(keypoints_amount,3,)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1, activation='sigmoid'),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "        )\n",
    "    return model\n",
    "\n",
    "model = create_and_compile_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej zachodzi trenowanie modelu. Znaczna część poniższego bloku służy obliczeniu metryk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Get average of 5 runs:\n",
    "metrics_values={\n",
    "    'loss':[],\n",
    "    'MCC':[],\n",
    "    'accuracy':[],\n",
    "\n",
    "    'confusion_matrix':[],\n",
    "\n",
    "    'negative_precision':[],\n",
    "    'negative_recall':[],\n",
    "    'negative_f1':[],\n",
    "    'positive_precision':[],\n",
    "    'positive_recall':[],\n",
    "    'positive_f1':[],\n",
    "\n",
    "}\n",
    "\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', \n",
    "                                              patience=10, verbose=1,)\n",
    "for i in range(repeat):\n",
    "    model = create_and_compile_model()\n",
    "    history = model.fit(train_dataset, \n",
    "                    epochs=200,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=[earlystopping],        \n",
    "         )\n",
    "    raw_y_pred = model.predict(test_dataset) #get predictions\n",
    "    raw_y_true = np.array(list(test_dataset.unbatch().map(lambda keypoints, label: label).as_numpy_iterator())) #get true labels\n",
    "    #Convert predictions into True or False\n",
    "    y_pred = tf.math.greater(raw_y_pred,threshold)\n",
    "    #Quickly convert 1 or 0 to True or False for true labels\n",
    "    y_true = tf.math.greater(raw_y_true, 0.5)\n",
    "\n",
    "    metrics_values['MCC'].append(matthews_corrcoef(y_true, y_pred))\n",
    "    metrics_values['confusion_matrix'].append(confusion_matrix(y_true, y_pred, normalize='all'))\n",
    "\n",
    "    cr = classification_report(y_true, y_pred, output_dict=True)\n",
    "    metrics_values['negative_precision'].append(cr['False']['precision'])\n",
    "    metrics_values['negative_recall'].append(cr['False']['recall'])\n",
    "    metrics_values['negative_f1'].append(cr['False']['f1-score'])\n",
    "    metrics_values['positive_precision'].append(cr['True']['precision'])\n",
    "    metrics_values['positive_recall'].append(cr['True']['recall'])\n",
    "    metrics_values['positive_f1'].append(cr['True']['f1-score'])\n",
    "    metrics_values['accuracy'].append(cr['accuracy'])\n",
    "\n",
    "    loss_fn = BinaryCrossentropy()\n",
    "    metrics_values['loss'].append(loss_fn(raw_y_true, raw_y_pred).numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liczona jest średnia metryk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map dict into average values\n",
    "print(\"pos support: \", list(raw_y_true).count(1))\n",
    "print(\"neg support: \",list(raw_y_true).count(0))\n",
    "average_metrics_values = {k:np.mean(v, axis=0) for k,v in metrics_values.items()}\n",
    "for k,v in average_metrics_values.items():\n",
    "    if(k!='confusion_matrix'):\n",
    "        v=round(v, 3)\n",
    "    print(v,\"-\", k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej wyświetli się kilkanaście wykresów oceniających model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykresy Loss od epoki oraz Accuracy od epoki jest stworzony tylko wyłącznie na podstawie ostatniego trenowania, lecz tyle powinno wystarczyć by zobaczyć jak się wykres ogólnie układa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training history to see whether you're overfitting.\n",
    "\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'VAL'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "\n",
    "labels = [\"Straight \\n (Neg)\", \"Slouching \\n (Pos)\"]\n",
    "cm=average_metrics_values['confusion_matrix']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej jest wykres, który pokazuje jak dokładność, MCC i precyzja się zmieniają w zależności od ustawionego progu (thresholdu). Służyło ku zbadaniu najlepszego progu dla programu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholds = np.arange(0.05, 1, 0.01)\n",
    "accuracies = []\n",
    "precisions = []\n",
    "mccs=[]\n",
    "for threshold in thresholds:\n",
    "    y_pred = tf.math.greater(raw_y_pred,threshold)\n",
    "    y_true = tf.math.greater(raw_y_true,threshold)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    precisions.append(precision)\n",
    "\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    mccs.append(mcc)\n",
    "\n",
    "plt.axvline(x=0.8, color='orange', linestyle='--')\n",
    "plt.axhline(y=0.936, color='orange', linestyle='--')\n",
    "plt.plot(thresholds, accuracies, label=\"accuracy\")\n",
    "plt.plot(thresholds, precisions, label=\"precision\")\n",
    "plt.plot(thresholds, mccs, label=\"mcc\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Accuracy & Precision & MCC against Threshold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def display_cm(y_true, y_pred, threshold=0.5, normalize=None):\n",
    "    labels = [\"Straight \\n (Neg)\", \"Slouching \\n (Pos)\"]\n",
    "    y_pred = tf.math.greater(y_pred,threshold)\n",
    "    y_true = tf.math.greater(y_true,threshold)\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels,)\n",
    "    disp.plot()\n",
    "display_cm(raw_y_true, raw_y_pred, threshold=0.8, normalize='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_tested = 0.8\n",
    "\n",
    "max_accuracy=round(max(accuracies), 3)\n",
    "print(\"Max accuracy: \", max_accuracy)\n",
    "\n",
    "threshold_0_8_index=np.where(np.round(thresholds,2)==threshold_tested)[0][0]\n",
    "\n",
    "accuracy_for_tested_threshold=round(accuracies[threshold_0_8_index], 3)\n",
    "precision_for_tested_threshold=round(precisions[threshold_0_8_index], 3)\n",
    "print(\"Tested threshold\", threshold_tested)\n",
    "print(\"Accuracy for tested threshold \", accuracy_for_tested_threshold)\n",
    "print(\"Precision for tested threshold \", precision_for_tested_threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapisywanie modelu: proszę odkomentować poniższy kod w celu zapisania modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path='../program_output/classification_model_tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "675fecd242027b04028c945ed6bf336b84b63c0d3fb3b96ac16c2bd160a786e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
